{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a32ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [46 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-39\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet transformers==2.9.0\n",
    "!pip install --quiet nltk==3.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0aea7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd0e2576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da51f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"shubham loves to watch cricket during his free time\"\n",
    "sentence2 = \"shubham is annoyed by a cricket in his room\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cdf9e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('cricket.n.01') :  leaping insect; male makes chirping noises by rubbing the forewings together \n",
      "\n",
      "Synset('cricket.n.02') :  a game played with a ball and bat by two teams of 11 players; teams take turns trying to score runs \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example of a word with two different senses\n",
    "original_word = \"cricket\"\n",
    "\n",
    "syns = wn.synsets(original_word,'n')\n",
    "\n",
    "for syn in syns:\n",
    "  print (syn, \": \",syn.definition(),\"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60f7427",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36516\\3118678049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0msynset_to_use\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mdistractors_calculated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_distractors_wordnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynset_to_use\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moriginal_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'original_word' is not defined"
     ]
    }
   ],
   "source": [
    "# Distractors from Wordnet\n",
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "\n",
    "synset_to_use = wn.synsets(original_word,'n')[0]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "\n",
    "print (\"\\noriginal word: \",original_word.capitalize())\n",
    "print (distractors_calculated)\n",
    "\n",
    "\n",
    "original_word = \"cricket\"\n",
    "synset_to_use = wn.synsets(original_word,'n')[1]\n",
    "distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n",
    "\n",
    "print (\"\\noriginal word: \",original_word.capitalize())\n",
    "print (distractors_calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71fc2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "bert_wsd_pytorch = 'C:\\\\Users\\\\shubh\\\\Downloads\\\\bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6.zip'\n",
    "extract_directory = 'C:\\\\Users\\\\shubh\\\\Downloads\\\\'\n",
    "\n",
    "extracted_folder = bert_wsd_pytorch.replace(\".zip\",\"\")\n",
    "\n",
    "#  If unzipped folder exists don't unzip again.\n",
    "if not os.path.isdir(extracted_folder):\n",
    "  with zipfile.ZipFile(bert_wsd_pytorch, 'r') as zip_ref:\n",
    "      zip_ref.extractall(extract_directory)\n",
    "else:\n",
    "  print (extracted_folder,\" is extracted already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce2d5395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n",
      "     -------------------------------------- 172.4/172.4 MB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in e:\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: networkx in e:\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76a55a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\shubh\\Downloads\\bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6 were not used when initializing BertWSD: ['similarity_linear.weight', 'similarity_loss_factor', 'similarity_linear.bias', 'ranking_loss_factor']\n",
      "- This IS expected if you are initializing BertWSD from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertWSD from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertWSD(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30523, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (ranking_linear): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import BertModel, BertConfig, BertPreTrainedModel, BertTokenizer\n",
    "\n",
    "class BertWSD(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.ranking_linear = torch.nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "# def _forward(args, model, batch):\n",
    "#     batch = tuple(t.to(args.device) for t in batch)\n",
    "#     outputs = model.bert(input_ids=batch[0], attention_mask=batch[1], token_type_ids=batch[2])\n",
    "\n",
    "#     return model.dropout(outputs[1])\n",
    "    \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dir = 'C:\\\\Users\\\\shubh\\\\Downloads\\\\bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6'\n",
    "\n",
    "\n",
    "model = BertWSD.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "# add new special token\n",
    "if '[TGT]' not in tokenizer.additional_special_tokens:\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})\n",
    "    assert '[TGT]' in tokenizer.additional_special_tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f057356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "GlossSelectionRecord = namedtuple(\"GlossSelectionRecord\", [\"guid\", \"sentence\", \"sense_keys\", \"glosses\", \"targets\"])\n",
    "BertInput = namedtuple(\"BertInput\", [\"input_ids\", \"input_mask\", \"segment_ids\", \"label_id\"])\n",
    "\n",
    "\n",
    "\n",
    "def _create_features_from_records(records, max_seq_length, tokenizer, cls_token_at_end=False, pad_on_left=False,\n",
    "                                  cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
    "                                  sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                                  cls_token_segment_id=1, pad_token_segment_id=0,\n",
    "                                  mask_padding_with_zero=True, disable_progress_bar=False):\n",
    "    \"\"\" Convert records to list of features. Each feature is a list of sub-features where the first element is\n",
    "        always the feature created from context-gloss pair while the rest of the elements are features created from\n",
    "        context-example pairs (if available)\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for record in tqdm(records, disable=disable_progress_bar):\n",
    "        tokens_a = tokenizer.tokenize(record.sentence)\n",
    "\n",
    "        sequences = [(gloss, 1 if i in record.targets else 0) for i, gloss in enumerate(record.glosses)]\n",
    "\n",
    "        pairs = []\n",
    "        for seq, label in sequences:\n",
    "            tokens_b = tokenizer.tokenize(seq)\n",
    "\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "            # The convention in BERT is:\n",
    "            # (a) For sequence pairs:\n",
    "            #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "            #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "            #\n",
    "            # Where \"type_ids\" are used to indicate whether this is the first\n",
    "            # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "            # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "            # embedding vector (and position vector). This is not *strictly* necessary\n",
    "            # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "            # it easier for the model to learn the concept of sequences.\n",
    "            #\n",
    "            # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "            # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "            # the entire model is fine-tuned.\n",
    "            tokens = tokens_a + [sep_token]\n",
    "            segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "            tokens += tokens_b + [sep_token]\n",
    "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "            if cls_token_at_end:\n",
    "                tokens = tokens + [cls_token]\n",
    "                segment_ids = segment_ids + [cls_token_segment_id]\n",
    "            else:\n",
    "                tokens = [cls_token] + tokens\n",
    "                segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            padding_length = max_seq_length - len(input_ids)\n",
    "            if pad_on_left:\n",
    "                input_ids = ([pad_token] * padding_length) + input_ids\n",
    "                input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "                segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            else:\n",
    "                input_ids = input_ids + ([pad_token] * padding_length)\n",
    "                input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "                segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            pairs.append(\n",
    "                BertInput(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label)\n",
    "            )\n",
    "\n",
    "        features.append(pairs)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d372d2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tabulate import tabulate\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "\n",
    "# Variables\n",
    "MAX_SEQ_LENGTH = 128\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def get_sense(sent):\n",
    "    re_result = re.search(r\"\\[TGT\\](.*)\\[TGT\\]\", sent)\n",
    "    if re_result is None:\n",
    "        print(\"\\nIncorrect input format. Please try again.\")\n",
    "        return None, None, None\n",
    "\n",
    "    ambiguous_word = re_result.group(1).strip()\n",
    "\n",
    "    results = dict()\n",
    "\n",
    "    wn_pos = wn.NOUN\n",
    "    for i, synset in enumerate(set(wn.synsets(ambiguous_word, pos=wn_pos))):\n",
    "        results[synset] = synset.definition()\n",
    "\n",
    "    if len(results) == 0:\n",
    "        return None, None, ambiguous_word\n",
    "\n",
    "    sense_keys = []\n",
    "    definitions = []\n",
    "    for sense_key, definition in results.items():\n",
    "        sense_keys.append(sense_key)\n",
    "        definitions.append(definition)\n",
    "\n",
    "    # Create input features\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    for record in zip(sense_keys, definitions):\n",
    "        input_encoding = tokenizer.encode_plus(\n",
    "            record[0].name(),\n",
    "            record[1],\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids.append(input_encoding[\"input_ids\"])\n",
    "        attention_masks.append(input_encoding[\"attention_mask\"])\n",
    "        token_type_ids.append(input_encoding[\"token_type_ids\"])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0).to(DEVICE)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0).to(DEVICE)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_masks,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        logits = outputs[1]\n",
    "        scores = softmax(logits, dim=1)\n",
    "\n",
    "        preds = sorted(zip(sense_keys, definitions, scores[:, 1]), key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "    sense = preds[0][0]\n",
    "    meaning = preds[0][1]\n",
    "    return sense, meaning, ambiguous_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4323f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Srivatsan loves to watch **cricket** during his free time\n",
      "Synset('cricket.n.01')\n",
      "leaping insect; male makes chirping noises by rubbing the forewings together\n",
      "\n",
      "-------------------------------\n",
      "Srivatsan is annoyed by a **cricket** in his room\n",
      "Synset('cricket.n.01')\n",
      "leaping insect; male makes chirping noises by rubbing the forewings together\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"Srivatsan loves to watch **cricket** during his free time\"\n",
    "sentence_for_bert = sentence1.replace(\"**\",\" [TGT] \")\n",
    "sentence_for_bert = \" \".join(sentence_for_bert.split())\n",
    "sense,meaning,answer = get_sense(sentence_for_bert)\n",
    "\n",
    "print (sentence1)\n",
    "print (sense)\n",
    "print (meaning)\n",
    "\n",
    "sentence2 = \"Srivatsan is annoyed by a **cricket** in his room\"\n",
    "sentence_for_bert = sentence2.replace(\"**\",\" [TGT] \")\n",
    "sentence_for_bert = \" \".join(sentence_for_bert.split())\n",
    "sense,meaning,answer = get_sense(sentence_for_bert)\n",
    "\n",
    "print (\"\\n-------------------------------\")\n",
    "print (sentence2)\n",
    "print (sense)\n",
    "print (meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "998f0169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d09280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
    "question_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "def get_question(sentence, answer):\n",
    "    text = \"context: {} answer: {} </s>\".format(sentence, answer)\n",
    "    max_len = 256\n",
    "    encoding = question_tokenizer.encode_plus(text, max_length=max_len, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "    outs = question_model.generate(input_ids=input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   early_stopping=True,\n",
    "                                   num_beams=5,\n",
    "                                   num_return_sequences=1,\n",
    "                                   no_repeat_ngram_size=2,\n",
    "                                   max_length=200)\n",
    "\n",
    "    dec = [question_tokenizer.decode(ids) for ids in outs]\n",
    "    Question = dec[0].replace(\"question:\", \"\")\n",
    "    Question = Question.strip()\n",
    "    return Question\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f697902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>  What sport does shubham enjoy watching?</s>\n",
      "\n",
      "**************************************\n",
      "\n",
      "<pad>  What type of cricket is in shubham's room?</s>\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"shubham loves to watch **cricket** during his free time\"\n",
    "sentence2 = \"shubham is annoyed by a **cricket** in his room\"\n",
    "\n",
    "answer = \"cricket\"\n",
    "\n",
    "sentence_for_T5 = sentence1.replace(\"**\", \" \")\n",
    "sentence_for_T5 = \" \".join(sentence_for_T5.split())\n",
    "ques = get_question(sentence_for_T5, answer)\n",
    "print(ques)\n",
    "\n",
    "print(\"\\n**************************************\\n\")\n",
    "\n",
    "sentence_for_T5 = sentence2.replace(\"**\", \" \")\n",
    "sentence_for_T5 = \" \".join(sentence_for_T5.split())\n",
    "ques = get_question(sentence_for_T5, answer)\n",
    "print(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ca6880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "E:\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>  What sport does shubham enjoy watching?</s>\n",
      "cricket\n",
      "['Grasshopper']\n",
      "leaping insect; male makes chirping noises by rubbing the forewings together\n",
      "\n",
      "\n",
      "<pad>  What type of cricket is in shubham's room?</s>\n",
      "cricket\n",
      "['Grasshopper']\n",
      "leaping insect; male makes chirping noises by rubbing the forewings together\n"
     ]
    }
   ],
   "source": [
    "def getMCQs(sent):\n",
    "  sentence_for_bert = sent.replace(\"**\",\" [TGT] \")\n",
    "  sentence_for_bert = \" \".join(sentence_for_bert.split())\n",
    "  # try:\n",
    "  sense,meaning,answer = get_sense(sentence_for_bert)\n",
    "  if sense is not None:\n",
    "    distractors = get_distractors_wordnet(sense,answer)\n",
    "  else: \n",
    "    distractors = [\"Word not found in Wordnet. So unable to extract distractors.\"]\n",
    "  sentence_for_T5 = sent.replace(\"**\",\" \")\n",
    "  sentence_for_T5 = \" \".join(sentence_for_T5.split()) \n",
    "  ques = get_question(sentence_for_T5,answer)\n",
    "  return ques,answer,distractors,meaning\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "question,answer,distractors,meaning = getMCQs(sentence1)\n",
    "print (question)\n",
    "print (answer)\n",
    "print (distractors)\n",
    "print (meaning)\n",
    "\n",
    "print (\"\\n\")\n",
    "question,answer,distractors,meaning = getMCQs(sentence2)\n",
    "print (question)\n",
    "print (answer)\n",
    "print (distractors)\n",
    "print (meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9697382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<pad>  What sport does shubham enjoy watching?</s>\n",
      "cricket\n",
      "['Grasshopper']\n",
      "leaping insect; male makes chirping noises by rubbing the forewings together\n",
      "\n",
      "\n",
      "<pad>  What type of cricket is in shubham's room?</s>\n",
      "cricket\n",
      "['Grasshopper']\n",
      "leaping insect; male makes chirping noises by rubbing the forewings together\n"
     ]
    }
   ],
   "source": [
    "def getMCQs(sent):\n",
    "    sentence_for_bert = sent.replace(\"**\", \" [TGT] \")\n",
    "    sentence_for_bert = \" \".join(sentence_for_bert.split())\n",
    "    sense, meaning, answer = get_sense(sentence_for_bert)\n",
    "    if sense is not None:\n",
    "        distractors = get_distractors_wordnet(sense, answer)\n",
    "    else:\n",
    "        distractors = [\"Word not found in Wordnet. So unable to extract distractors.\"]\n",
    "    sentence_for_T5 = sent.replace(\"**\", \" \")\n",
    "    sentence_for_T5 = \" \".join(sentence_for_T5.split())\n",
    "    ques = get_question(sentence_for_T5, answer)\n",
    "    return ques, answer, distractors, meaning\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "question, answer, distractors, meaning = getMCQs(sentence1)\n",
    "print(question)\n",
    "print(answer)\n",
    "print(distractors)\n",
    "print(meaning)\n",
    "\n",
    "print(\"\\n\")\n",
    "question, answer, distractors, meaning = getMCQs(sentence2)\n",
    "print(question)\n",
    "print(answer)\n",
    "print(distractors)\n",
    "print(meaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f84168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<pad>  Where did John go to cry?</s>\n",
      "bank\n",
      "['Ascent', 'Canyonside', 'Coast', 'Descent', 'Escarpment', 'Hillside', 'Mountainside', 'Piedmont', 'Ski Slope']\n",
      "a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n"
     ]
    }
   ],
   "source": [
    "# More examples\n",
    "\n",
    "sentence = \"John went to river **bank** to cry\"\n",
    "# sentence = \"John went to deposit money in the **bank**\"\n",
    "\n",
    "# sentence = \"John bought a **mouse** for his computer.\"\n",
    "# sentence = \"John saw a **mouse** under his bed.\"\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "question,answer,distractors,meaning = getMCQs(sentence)\n",
    "print (question)\n",
    "print (answer)\n",
    "print (distractors)\n",
    "print (meaning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
